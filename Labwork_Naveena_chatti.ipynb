{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjwdpwuckxjg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LabWork:Model Initialization\n",
        "\n",
        "Welcome to the fourth (and last) programming assignment of Course 2!\n",
        "\n",
        "In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n",
        "- To implement sentiment analysis, you can go beyond counting the number of positive words and negative words.\n",
        "- You can find a way to represent each word numerically, by a vector.\n",
        "- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures.\n",
        "\n",
        "In this assignment, you will explore a classic way of generating word embeddings or representations.\n",
        "- You will implement a famous model called the continuous bag of words (CBOW) model.\n",
        "\n",
        "By completing this assignment you will:\n",
        "\n",
        "- Train word vectors from scratch.\n",
        "- Learn how to create batches of data.\n",
        "- Understand how backpropagation works.\n",
        "- Plot and visualize your learned word vectors.\n",
        "\n",
        "Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing.\n",
        "\n",
        "## Important Note on Submission to the AutoGrader\n",
        "\n",
        "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
        "\n",
        "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
        "2. You have not added any _extra_ code cell(s) in the assignment.\n",
        "3. You have not changed any of the function parameters.\n",
        "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
        "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
        "\n",
        "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/probabilistic-models-in-nlp/supplement/saGQf/how-to-refresh-your-workspace)."
      ],
      "metadata": {
        "id": "P7iYTU_BpGFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python libraries and helper functions (in utils2)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
        "import w4_unittest\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGHBk9CvpCBA",
        "outputId": "7f1ede9a-8aff-4dd3-8886-dcefc3abaec4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sentence tokenizer\n",
        "nltk.data.path.append('.')"
      ],
      "metadata": {
        "id": "sOdTDlX7lAue"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load, tokenize and process the data\n",
        "import re                                                           #  Load the Regex-modul\n",
        "with open('/content/shakespeare-1.txt') as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz8sfPQwlLD2",
        "outputId": "cbe7b9d4-ccd9-4576-c752-06860ca40f24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 60976 \n",
            " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \",len(fdist) )\n",
        "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oOzAZillMoR",
        "outputId": "289f7c04-ddce-4b39-dde6-4df50d4fbd74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  5775\n",
            "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U_UcgfqlUBQ",
        "outputId": "8a574347-27f3-4c51-dfae-67afb7004d0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary:  5775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of word to index mapping\n",
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md0Mk9x7nMlq",
        "outputId": "f5aefff1-c8ca-4613-c159-72334f26a77f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index of the word 'king' :   2744\n",
            "Word which has index 2743:   kinds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the model"
      ],
      "metadata": {
        "id": "ZCxml1KUpk-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
        "# UNQ_C1 GRADED FUNCTION: initialize_model\n",
        "def initialize_model(N,V, random_seed=1):\n",
        "    '''\n",
        "    Inputs:\n",
        "        N:  dimension of hidden vector\n",
        "        V:  dimension of vocabulary\n",
        "        random_seed: random seed for consistent results in the unit tests\n",
        "     Outputs:\n",
        "        W1, W2, b1, b2: initialized weights and biases\n",
        "    '''\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    np.random.seed(random_seed)\n",
        "    # W1 has shape (N,V)\n",
        "    W1 = np.random.rand(N,V)\n",
        "\n",
        "    # W2 has shape (V,N)\n",
        "    W2 = np.random.rand(V,N)\n",
        "\n",
        "    # b1 has shape (N,1)\n",
        "    b1 = np.random.rand(N,1)\n",
        "\n",
        "    # b2 has shape (V,1)\n",
        "    b2 = np.random.rand(V,1)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "4uH1crqAnOTz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function example.\n",
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
        "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
        "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNA_xPmmoYPw",
        "outputId": "ddc1479e-10ba-46e2-9667-f1a64e62c2ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmp_W1.shape: (4, 10)\n",
            "tmp_W2.shape: (10, 4)\n",
            "tmp_b1.shape: (4, 1)\n",
            "tmp_b2.shape: (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Expected Output\n",
        "\n",
        "```CPP\n",
        "tmp_W1.shape: (4, 10)\n",
        "tmp_W2.shape: (10, 4)\n",
        "tmp_b1.shape: (4, 1)\n",
        "tmp_b2.shape: (10, 1)\n",
        "```\n"
      ],
      "metadata": {
        "id": "dxIYQXqPouxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "w4_unittest.test_initialize_model(initialize_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNiD35FvoZZG",
        "outputId": "a3302d3b-c3cd-47e3-c8f2-7784c24b85db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYh8nUpZo-Qo"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}